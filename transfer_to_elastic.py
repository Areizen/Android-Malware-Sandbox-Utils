#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import print_function

from os.path import dirname, basename, abspath
from datetime import datetime
import logging
import sys
import argparse

import sqlite3

from elasticsearch import Elasticsearch
from elasticsearch.exceptions import TransportError
from elasticsearch.helpers import bulk, streaming_bulk

def create_analysis_index(client, index):
    '''
    Create mapping associated with the analysis
    '''
    application_mapping = {
        "properties": {
            "package": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "sha256": {"type": "text", "fields": {"keyword": {"type": "keyword"}}}
        }
    }

    string_mapping = {
        "properties": {
            "value" : {"type": "text", "fields": {"keyword": {"type": "keyword"}}}
        }
    }

    create_index_body = {
        "settings": {
            # just one shard, no replicas for testing
            "number_of_shards": 1,
            "number_of_replicas": 0,
            # custom analyzer for analyzing file paths
        },
        "mappings": {
            "properties": {
                "application": application_mapping,
                "string" : string_mapping,
                "timestamp":{"type":"date"}
            }
        },
    }

    # create empty index
    try:
        client.indices.create(index=index, body=create_index_body)
    except TransportError as e:
        # ignore already existing index
        if e.error == "resource_already_exists_exception":
            pass
        else:
            raise

def parse_rows(rows):
    '''
    Parse each row of the sqlite database
    '''
    for row in rows:
        string_value = row[1][:3000]
        string = { 'id' : row[0], 'value': string_value }
        application = { 'id' : row[4], 'package': row[5], 'sha256' : row[6] }
        yield {
            "timestamp" : datetime.strptime(row[2], '%Y-%m-%d %H:%M:%S.%f'),
            "application" : application,
            "string" : string
            }

def process_data(client, database, index, analysis_id):
    '''
    Gather data from sqlite database
    '''
    conn = sqlite3.connect(database)
    c = conn.cursor()
    rows = c.execute('''
    select 
    string.id, string.value, string.date, string.application_id, application.id, application.package, application.path, application.sha256, application.analysis_id
    from string 
    join application
    join analysis
    where string.application_id = application.id and analysis.id = application.analysis_id and analysis.id=?;
     ''', (analysis_id))

    for ok, result in streaming_bulk(
        client,
        parse_rows(rows),
        index=index,
        chunk_size=50,  # keep the batch sizes small for appearances only
            ):
    
        action, result = result.popitem()
        print(action,result)

if __name__ == "__main__":
    # get trace logger and set level
    tracer = logging.getLogger("elasticsearch.trace")
    tracer.setLevel(logging.INFO)
    tracer.addHandler(logging.FileHandler("/tmp/es_trace.log"))

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-H",
        "--host",
        action="store",
        default="localhost:9200",
        help="The elasticsearch host you wish to connect to. (Default: localhost:9200)",
    )

    parser.add_argument(
        "-D",
        "--database",
        action="store",
        default="foo.db",
        help="The database where to extract the data. (Default: foo.db)",
    )

    parser.add_argument(
        "-i",
        "--index",
        action="store",
        default="analysis",
        help="The index where data will be stored. (Default: analysis)",
    )
    parser.add_argument(
        "-a",
        "--analysis_id",
        action="store",
        help="The id of the analysis to send",
    )
    args = parser.parse_args()

    # instantiate es client, connects to localhost:9200 by default
    es = Elasticsearch(args.host)

    # we load the repo and all commits
    create_analysis_index(es,index=args.index)

    #add data
    process_data(es,args.database,args.index, args.analysis_id)